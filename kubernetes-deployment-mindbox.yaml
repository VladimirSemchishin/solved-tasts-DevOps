 ---
### Name space
apiVersion: v1
kind: NameSpace
metadata:			#Создал пространство имен (для подов)
  name: web-project
  
 ---
### Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-project-dep			# имя процесса развертывания
  namespace: deploy       # имя пространства имен
  labels:
    app: web-project-dep  # метка
spec:
  replicas: 4  # количество подов, которое будет запущенно одновременно в момент развертывания, указал 4, поскольку это то кол-во, которое выдержит пиковую нагрузку. Можно указать и 1, но деплоить ночью, когда нагрузка минимальна.
  
  strategy:				# cтратегия обновления подов
    type: RollingUpdate			# Этот тип означает плавную замену подов, за раз может обновляться только один под (создается новый под при работающем старом, после введения его в работу удаляется старый),
      rollingUpdate:		 
        maxUnavaible: 1			# чтобы не произошла ситуация, при которой приложение недоступно из-за обновления всех подов, по этому обновлять по одному поду 
  
  selector:
    matchLables:            # вызов пода по метке, чтобы deployment обнаружил, какими подами нужно управлять
      app: web-project-pod		  
  template:				# описание шаблона пода 
    metadata:
      lables:
        app: deploy
    spec:		
      containers:			# описание контейнера, который хранится в поде
      - name: image-ubuntu		# имя контейнера
        image: adress-image-ubuntu	# образ контейнера (адрес)
      	ports:
      	  containerPort: 8080		# порт контейнера
         name: http       # имя порта
        
        resources:			# количество ресурсов и лимиты
          requests:			
            memory: "128M"		# исзодя из опыта работы столько паяти и CPU хватает
            cpu: 0.1
          limits:			# лимиты я установил следующие поскольку
            memory: "150M"		# задача поставлена на максимально устойчивый деплоймент (запас не - не дорого и не помешает). Не ставлю лимит для времени использования ЦП, поскольку тебуется максимальная отказоустойчивость, и при повышенном потребеление будет создан новый под (на 50%), который снизит нагрузку на имеющиеся (что должно нормализовать уровень потребления)
                                                                 
      readinessProbe:	 		# проверка роботоспособности, если команда выдаст что 
        exen:				# кроме 0 (healthy), значит сайт unhealthy
          command:
          - cat
          - /tmp/health
        initialDelaySeconds: 10		# делаю первую проверку через 10 секунд, поскольку развертывание приложения занимает 5-10 секунд
        periodSeconds: 5		# после, проверка каждые 5 секунд


 ---
### LoadBalancer
apiVersion: v1
kind: Servise
metadata:
  name: load-balancer			# создаю балансировщик
  namespace: web-project
spec:
  ports:
  - port: 80         # это порт балансировщика			
    name: web-project-port
    targetPort: 8080   # это порт контейнера, на котором доступно приложение
  selector:
    app: web-project-pod
  type: LoadBalancer
  loadBalancerIP: 10.10.130.145		# заранее зарезервированный IP. Через него будет единый доступ ко всем сущестующим подам (развернутом на них приложении), в том числе и вне кластера.

 ---
### Horizontal Pod Autoscaler
apiVersion: autoscaling/v1		
kind: HorizontalPodAutoscaler
metadata:
  name: HPA				# Автоматическое масштабирование
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-project-dep
  minReplicas: 1			# минимально возможное количество подов
  maxReplicas: 4			# максимально возможное кол-во подов (даже в во время обновления подов, старый будет заменятся новым только при выполнении условия)
  metrics: 				# указываю, что при достижении нагрузки ЦП в 50% будет создан новый под.
  - type: Resource			  
    resource:
    - name: cpu
      target:
        type: Utilization
        averageUtilization: 50
